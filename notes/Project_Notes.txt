Describe the past process:

1. getting the data from one online database
- getting it with selenium and parsing the xml
- a lot of work but ok

2. adding to my postgresql

3. chunking and embedding

4. downloading and building a local llm
- adding a model from hugging_face: 4Q mistral

5. creating a bare bones react webapp
- backend in fastapi

6. problems with answer: 
chunk too big.
- divided in smaller chunks for quantized llm
- llm is specialized in instructions (mistral-7b-instruct-v0.1.Q4_K_M.gguf) so had to adapt prompt
- still getting gibberish, so changed to use ollama and nous-hermes2

7. Using ollama
- it got better results from a simple question
- could not get good results if question involved information spread between chunks
- changed embeddings

8. Benchmarked with one question in local ollama. Not clear which one is better

9. Created Docker file with only ollama llm iference
10. Uploaded DockerFile to DokcerHub

11. Created Runpod from dockerfile

12. Inference is taking now 8s instead of 60-300s

13. Working on improving embeddings:
- embedding chunks should overlap information or else they are too loose and disconnected
- trying to do embeddings with SentenceTransformer("BAAI/bge-large-en-v1.5") (vector size (1024))

14. System crashed. Embedding creation too resource expensive. Snap broke along with many apps (vscode, pgadmin, ....)
Was able to salvage the code, database was not affected but had to free up a lot of space (many llm images laying around) and reinstall a few apps
- moved to do embeddings with SentenceTransformer("BAAI/bge-small-en-v1.5") (vecotr size 384)

15. Data cleaning / exploration / preparation
- normalizing lcia and exchanges units
  - MUST CREATE columns normalized amound and normalized unit
- getting density information from other file

16. Created comparison components in frontend:
- select products, gets list.
- select indicators
- choose "compare"
- gets tables showing comparison for productor for each indicators

17. Prepare runpod with ollama in a way that I can switch models for benchmarking.
Installing ollama in runpod.io
ollama pull mistral
ollama pull llama3
ollama pull gemma:2b
(ollama pull qwen:1.8b)
(ollama pull phi3:mini)
ollama run mistral

18. Text extraction from pdf with pdfplumber
- gets text, differentiate between title, sections, etc...
- removes headers and footers
- saves tables in a csv in a way they can be referenced and embedded later

19. Text chunking for embedding:
- 3-4 sentences to begin with

20. Data cleaning
- translating and uniform category levels 1, 2 and 3
  - translated (total) 483 entries in chatGpt and used those to set category to product

21. Data exploration and cleaning:
- got flow properties and modified materials to material_properties
- now I can compare indicators accross pahses, categories and normalize values

22. Materials and use cases are not reliably found in the EPD information. I will loop through the products and ask the llm to infer it from the text if available.
- a lot of prompt engineering, translating text from german to english, summarizing long information...



- extra indicators: lcia (IRP, GWP, EP) and exchanges(SF)

Types of questions:
Compare 2 or more products on indicators
Find out alternative products for same use case
Which product types are good for my building design 
- find product for my use case
Indicators that are important
- how to decide if no clear product is better
How to use the product in a sustainable way?
How circular is this product?




f63ac879-fa7d-4f91-813e-e816cbdf1927_00.00.025

SELECT *
FROM products p
WHERE NOT EXISTS (
    SELECT 1
    FROM flow_properties fp
    WHERE fp.process_id = p.process_id
      AND fp.is_reference = TRUE
);



SELECT table_schema, table_name, column_name, data_type
FROM information_schema.columns
WHERE table_schema = 'public'
ORDER BY table_schema, table_name, ordinal_position;
